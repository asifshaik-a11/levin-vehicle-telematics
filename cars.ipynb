{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "182a89a7-8486-48ae-b547-eb8d979fee5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF test F1: 0.5714285714285714\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.93      0.94        27\n",
      "           1       0.50      0.67      0.57         3\n",
      "\n",
      "    accuracy                           0.90        30\n",
      "   macro avg       0.73      0.80      0.76        30\n",
      "weighted avg       0.92      0.90      0.91        30\n",
      "\n",
      "GB test F1: 0.4444444444444444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.85      0.90        27\n",
      "           1       0.33      0.67      0.44         3\n",
      "\n",
      "    accuracy                           0.83        30\n",
      "   macro avg       0.65      0.76      0.67        30\n",
      "weighted avg       0.90      0.83      0.86        30\n",
      "\n",
      "             feature  importance\n",
      "16  harsh_score_like    0.011667\n",
      "8          min_decel    0.010000\n",
      "0       rows_first80    0.006667\n",
      "13  heavy_decel_rate    0.001667\n",
      "1         mean_speed    0.000000\n",
      "4   pct_time_over_80    0.000000\n",
      "5         stop_ratio    0.000000\n",
      "3       median_speed    0.000000\n",
      "2          max_speed    0.000000\n",
      "9          max_decel    0.000000\n",
      "6         idle_ratio    0.000000\n",
      "11          max_jerk   -0.006667\n"
     ]
    }
   ],
   "source": [
    "# Full reproducible pipeline \n",
    "import pandas as pd, numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "from sklearn.inspection import permutation_importance\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "path = r'C:/Users/Asus/Downloads/allcars.csv'   # <-- use your local path\n",
    "df = pd.read_csv(path)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "if 'timeStamp' in df.columns:\n",
    "    df = df.rename(columns={'timeStamp':'timestamp'})\n",
    "if 'tripID' in df.columns:\n",
    "    df = df.rename(columns={'tripID':'trip_id'})\n",
    "\n",
    "# parse timestamp strings like '46:00.0' to seconds\n",
    "def parse_ts(s):\n",
    "    try:\n",
    "        parts = str(s).split(':')\n",
    "        if len(parts)==2:\n",
    "            m = float(parts[0]); sec = float(parts[1])\n",
    "            return m*60 + sec\n",
    "        elif len(parts)==3:\n",
    "            h=float(parts[0]); m=float(parts[1]); sec=float(parts[2])\n",
    "            return h*3600 + m*60 + sec\n",
    "        else:\n",
    "            return float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "df['ts_seconds'] = df['timestamp'].apply(parse_ts)\n",
    "\n",
    "# ensure 'speed' column exists\n",
    "if 'speed' not in df.columns and 'gps_speed' in df.columns:\n",
    "    df['speed'] = df['gps_speed']\n",
    "if 'speed' not in df.columns:\n",
    "    raise ValueError(\"No speed column found.\")\n",
    "\n",
    "# compute dt, decel, jerk per trip (safe dt handling)\n",
    "def compute_derivs(g):\n",
    "    g = g.sort_values('ts_seconds').reset_index(drop=True)\n",
    "    g['dt'] = g['ts_seconds'].diff().fillna(1.0)\n",
    "    g.loc[g['dt'] <= 0, 'dt'] = 0.5\n",
    "    g['speed_mps'] = g['speed'].fillna(0) * (1000/3600)\n",
    "    g['dv'] = g['speed_mps'].diff().fillna(0)\n",
    "    g['decel'] = (g['dv'] / g['dt']).clip(-50,50)\n",
    "    g['jerk'] = g['decel'].diff().fillna(0) / g['dt']\n",
    "    g['jerk'] = g['jerk'].clip(-100,100)\n",
    "    return g\n",
    "\n",
    "df = df.groupby('trip_id', group_keys=False).apply(compute_derivs)\n",
    "\n",
    "# label: harsh braking if any decel <= -3.0 m/s^2 in last 20% rows of trip\n",
    "def label_trip(g, thr=-3.0):\n",
    "    g = g.sort_values('ts_seconds')\n",
    "    n = len(g)\n",
    "    cutoff = int(np.ceil(0.8*n))\n",
    "    last = g.iloc[cutoff:]\n",
    "    return int((last['decel'] <= thr).any())\n",
    "\n",
    "trip_labels = df.groupby('trip_id').apply(label_trip).rename('harsh_last20').reset_index()\n",
    "\n",
    "# aggregate features from first 80%\n",
    "def agg_feats(g):\n",
    "    g = g.sort_values('ts_seconds').reset_index(drop=True)\n",
    "    n = len(g); cutoff = int(np.ceil(0.8*n)); first=g.iloc[:cutoff]\n",
    "    res = {\n",
    "        'rows_first80': len(first),\n",
    "        'mean_speed': first['speed'].mean(),\n",
    "        'max_speed': first['speed'].max(),\n",
    "        'median_speed': first['speed'].median(),\n",
    "        'pct_time_over_80': (first['speed'] > 80).mean(),\n",
    "        'stop_ratio': (first['speed'] <= 1).mean(),\n",
    "        'idle_ratio': (first['speed'] < 5).mean(),\n",
    "        'mean_decel': first['decel'].mean(),\n",
    "        'min_decel': first['decel'].min(),\n",
    "        'max_decel': first['decel'].max(),\n",
    "        'mean_jerk': first['jerk'].mean(),\n",
    "        'max_jerk': first['jerk'].max(),\n",
    "        'heavy_decel_count': (first['decel'] <= -3.0).sum(),\n",
    "    }\n",
    "    res['heavy_decel_rate'] = res['heavy_decel_count'] / max(1, res['rows_first80'])\n",
    "    res['decel_std'] = first['decel'].std()\n",
    "    res['speed_std'] = first['speed'].std()\n",
    "    res['harsh_score_like'] = -res['min_decel'] * res['heavy_decel_rate'] + abs(res['mean_jerk'] if not pd.isna(res['mean_jerk']) else 0)\n",
    "    return pd.Series(res)\n",
    "\n",
    "trip_feats = df.groupby('trip_id').apply(agg_feats).reset_index()\n",
    "data = trip_feats.merge(trip_labels, on='trip_id', how='left')\n",
    "data['start_ts'] = df.groupby('trip_id')['ts_seconds'].min().values\n",
    "\n",
    "# clean inf / nan\n",
    "data.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "for c in data.columns:\n",
    "    if data[c].dtype.kind in 'fi':\n",
    "        data[c].fillna(data[c].median(), inplace=True)\n",
    "\n",
    "# save features\n",
    "data.to_csv('trip_level_features_labels.csv', index=False)\n",
    "\n",
    "# modeling\n",
    "features = [c for c in data.columns if c not in ['trip_id','harsh_last20','start_ts']]\n",
    "X = data[features]; y = data['harsh_last20'].astype(int)\n",
    "scaler = StandardScaler(); Xs = scaler.fit_transform(X)\n",
    "\n",
    "data = data.sort_values('start_ts').reset_index(drop=True)\n",
    "n = len(data); train_end=int(0.6*n); val_end=int(0.8*n)\n",
    "train = data.iloc[:train_end]; val = data.iloc[train_end:val_end]; test = data.iloc[val_end:]\n",
    "X_train = scaler.transform(train[features]); y_train = train['harsh_last20']\n",
    "X_test = scaler.transform(test[features]); y_test = test['harsh_last20']\n",
    "\n",
    "# RandomForest randomized search (n_iter=20)\n",
    "rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
    "param_rf = {'n_estimators':[50,100,150],'max_depth':[3,5,8,12,None],'min_samples_split':[2,5,10],'min_samples_leaf':[1,2,4],'max_features':['sqrt','log2',None]}\n",
    "rs_rf = RandomizedSearchCV(rf, param_rf, n_iter=20, scoring='f1', cv=3, random_state=42, n_jobs=-1)\n",
    "rs_rf.fit(X_train, y_train)\n",
    "best_rf = rs_rf.best_estimator_\n",
    "y_test_pred_rf = best_rf.predict(X_test)\n",
    "print(\"RF test F1:\", f1_score(y_test, y_test_pred_rf))\n",
    "print(classification_report(y_test, y_test_pred_rf))\n",
    "\n",
    "# GradientBoosting randomized search (n_iter=20)\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "param_gb = {'n_estimators':[50,100,150],'learning_rate':[0.01,0.05,0.1],'max_depth':[3,4,5],'min_samples_leaf':[1,2,4],'subsample':[0.6,0.8,1.0]}\n",
    "rs_gb = RandomizedSearchCV(gb, param_gb, n_iter=20, scoring='f1', cv=3, random_state=42, n_jobs=-1)\n",
    "rs_gb.fit(X_train, y_train)\n",
    "best_gb = rs_gb.best_estimator_\n",
    "y_test_pred_gb = best_gb.predict(X_test)\n",
    "print(\"GB test F1:\", f1_score(y_test, y_test_pred_gb))\n",
    "print(classification_report(y_test, y_test_pred_gb))\n",
    "\n",
    "# permutation importance\n",
    "perm = permutation_importance(best_gb, X_test, y_test, n_repeats=20, random_state=42, n_jobs=-1)\n",
    "imp_df = pd.DataFrame({'feature':features, 'importance':perm.importances_mean}).sort_values('importance', ascending=False)\n",
    "print(imp_df.head(12))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c7bc870f-8d0d-46e4-828e-293e03b5d1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%history -f cars.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9944ec96-6e96-4b8f-8692-296c707a4754",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:2025 conda]",
   "language": "python",
   "name": "conda-env-2025_conda-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
